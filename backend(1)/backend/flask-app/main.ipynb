{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: keyboard in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: ipyplot in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: click in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: IPython in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from ipyplot) (8.7.0)\n",
      "Requirement already satisfied: shortuuid in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipyplot) (1.0.11)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: backcall in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from IPython->ipyplot) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from IPython->ipyplot) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from IPython->ipyplot) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from IPython->ipyplot) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from IPython->ipyplot) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from IPython->ipyplot) (3.0.33)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->ipyplot) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from IPython->ipyplot) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from IPython->ipyplot) (5.6.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->IPython->ipyplot) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit<3.1.0,>=3.0.11->IPython->ipyplot) (0.2.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->IPython->ipyplot) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->IPython->ipyplot) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->IPython->ipyplot) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers pandas matplotlib nltk keyboard numpy ipyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading nps_chat: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('nps_chat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1'\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import sys\n",
    "import keyboard\n",
    "import json\n",
    "import ast\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import pickle\n",
    "import ipyplot\n",
    "\n",
    "\n",
    "\n",
    "model_id = \"rasta/distilbert-base-uncased-finetuned-fashion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)\n",
    "\n",
    "def classify(text):\n",
    "    preds = classifier(text, return_all_scores=True)\n",
    "    if preds[0][0]['score']  <= preds[0][1]['score']:\n",
    "        return \"Not Fashion\"\n",
    "    else:\n",
    "        return \"Fashion\"\n",
    "\n",
    "def attribute_extraction(txt):\n",
    "    tokenized = sent_tokenize(txt)\n",
    "\n",
    "    attributes = []\n",
    "    for i in tokenized:\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "\n",
    "    for i,w in enumerate(tagged) :\n",
    "        if w[1] in ['NN','NNS','RB'] :\n",
    "            ind =i\n",
    "            attr = w[0]\n",
    "            while tagged[ind-1][1] in ['JJ','VBN','NN','RB','VBD','EX']:\n",
    "                    attr = tagged[ind-1][0] + ' ' +  attr\n",
    "                    ind = ind - 1\n",
    "\n",
    "            if len(attr.split())==1 and txt.split()[0].lower()=='will':\n",
    "                attr = tagged[ind-1][0] + ' ' +  attr\n",
    "\n",
    "            if classify(attr) == 'Fashion':\n",
    "                attributes.append(attr)\n",
    "            for a in attributes:\n",
    "                for b in attributes:\n",
    "                    if (a!=b) and (a in b):\n",
    "                        attributes.remove(a)\n",
    "\n",
    "            for a in attributes:\n",
    "                if 'fit' in a :\n",
    "                    attributes = list(map(lambda x: x.replace(a, a.replace(' fit','')), attributes))\n",
    "                if 'match' in a :\n",
    "                    attributes = list(map(lambda x: x.replace(a, a.replace(' match','')), attributes))\n",
    "\n",
    "    return attributes\n",
    "\n",
    "\n",
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features\n",
    "\n",
    "featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
    "\n",
    "# 10% of the total data\n",
    "size = int(len(featuresets) * 0.1)\n",
    "\n",
    "# first 10% for test_set to check the accuracy, and rest 90% after the first 10% for training\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "# get the classifer from the training set\n",
    "classifiers = nltk.NaiveBayesClassifier.train(train_set)\n",
    "# to check the accuracy - 0.67\n",
    "# print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "question_types = [\"whQuestion\",\"ynQuestion\"]\n",
    "def is_ques_using_nltk(ques):\n",
    "    question_type = classifiers.classify(dialogue_act_features(ques))\n",
    "    return question_type in question_types\n",
    "\n",
    "\n",
    "question_pattern = [\"do i\", \"do you\", \"what\", \"who\", \"is it\", \"why\",\"would you\", \"how\",\"is there\",\n",
    "                    \"are there\", \"is it so\", \"is this true\" ,\"to know\", \"is that true\", \"are we\", \"am i\",\n",
    "                   \"question is\", \"tell me more\", \"can i\", \"can we\", \"tell me\", \"can you explain\",\n",
    "                   \"question\",\"answer\", \"questions\", \"answers\", \"ask\"]\n",
    "\n",
    "helping_verbs = [\"is\",\"am\",\"can\", \"are\", \"do\", \"does\"]\n",
    "# check with custom pipeline if still this is a question mark it as a question\n",
    "\n",
    "def is_question(question):\n",
    "    question = question.lower().strip()\n",
    "    if not is_ques_using_nltk(question):\n",
    "        is_ques = False\n",
    "        # check if any of pattern exist in sentence\n",
    "        for pattern in question_pattern:\n",
    "            is_ques  = pattern in question\n",
    "            if is_ques:\n",
    "                break\n",
    "\n",
    "        # there could be multiple sentences so divide the sentence\n",
    "        sentence_arr = question.split(\".\")\n",
    "        for sentence in sentence_arr:\n",
    "            if len(sentence.strip()):\n",
    "                # if question ends with ? or start with any helping verb\n",
    "                # word_tokenize will strip by default\n",
    "                first_word = nltk.word_tokenize(sentence)[0]\n",
    "                if sentence.endswith(\"?\") or first_word in helping_verbs:\n",
    "                    is_ques = True\n",
    "                    break\n",
    "        return is_ques\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "model_semantick_id = \"PriaPillai/distilbert-base-uncased-finetuned-query\"\n",
    "classifier_sem = pipeline(\"text-classification\", model=model_semantick_id)\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "verb_pattern = [ps.stem(i) for i in ['match', 'suit', 'fit', 'wear', 'pair']]\n",
    "# 'be', 'go', 'are'\n",
    "\n",
    "def semantic_check_hard_coded(txt):\n",
    "    tokenized = sent_tokenize(txt)\n",
    "    verbs = []\n",
    "\n",
    "    for i in tokenized:\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "\n",
    "    for i,w in enumerate(tagged) :\n",
    "        if w[1] in ['VB','VBD','VBN','VBG','VBP','VBZ'] :\n",
    "            verbs.append(ps.stem(w[0]))\n",
    "\n",
    "    for v in verbs:\n",
    "        if v in verb_pattern :\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def semantic_check(text):\n",
    "    if semantic_check_hard_coded(text):\n",
    "        return True\n",
    "    preds = classifier_sem(text, return_all_scores=True)\n",
    "    if preds[0][0]['score']  <= preds[0][1]['score']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def extraction_pipeline(query):\n",
    "    if not is_question(query):\n",
    "        message = \"I am not understanding you, please enter a question that is related to fashion\"\n",
    "        return message, []\n",
    "    elif not semantic_check(query) :\n",
    "        message = \"I am not sure to get your query can you please try again ?\"\n",
    "        return message, []\n",
    "    else:\n",
    "        return \"Working ...\",attribute_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.66.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.9.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "frame = pd.read_csv('image_id.csv')\n",
    "frame = frame.drop(columns=[\"Unnamed: 0\"])\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "def sample(x):\n",
    "    return data[\"Attributes\"][x]\n",
    "\n",
    "\n",
    "def extract_from_sample(i):\n",
    "    dic = eval(sample(i))\n",
    "    a = [dic[k]['attrs'] for k in dic.keys()]\n",
    "\n",
    "    occur = []\n",
    "    for i, obj in enumerate(a):\n",
    "        sent = ' '.join([d[0] for d in obj]) + ' ' + list(dic.keys())[i]\n",
    "        occur.append(sent)\n",
    "\n",
    "    return occur\n",
    "\n",
    "\n",
    "def extract_image(attr1, attr2, k):\n",
    "    match = []\n",
    "    a = 0\n",
    "    for i, d in enumerate(data['Attributes']):\n",
    "        l = extract_from_sample(i)\n",
    "        if (attr1 in l) and (attr2 in l):\n",
    "            matching_urls = list(frame[frame['id'] == i]['URL'])\n",
    "\n",
    "\n",
    "            if matching_urls:\n",
    "                match.append(matching_urls[0])\n",
    "            else:\n",
    "                # Handle the case when no URL is found for the current 'i'\n",
    "                match.append(\"No matching URL found for this ID\")\n",
    "\n",
    "            a = a + 1\n",
    "            if a == k:\n",
    "                break\n",
    "\n",
    "    if len(match) >= 1:\n",
    "        ipyplot.plot_images(match, max_images=20,\n",
    "                            img_width=150, show_url=False)\n",
    "    else:\n",
    "        print(\"No image found\")\n",
    "\n",
    "    return match\n",
    "\n",
    "# from simcse import SimCSE\n",
    "\n",
    "\n",
    "model_sentence_transformer = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "with open('index.pkl', 'rb') as f:\n",
    "    index = pickle.load(f)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# model_SIMCSE.index = index\n",
    "# items = index['sentences']\n",
    "items = index['sentences']\n",
    "\n",
    "\n",
    "def similar_items(attr, sentence_embeddings_tensor):\n",
    "    similar_items = []\n",
    "    attr_embedding = model_sentence_transformer.encode(\n",
    "        [attr], convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity between the attribute embedding and index embeddings\n",
    "    cosine_scores = util.pytorch_cos_sim(\n",
    "        attr_embedding, sentence_embeddings_tensor)[0]\n",
    "\n",
    "    # Find items with cosine similarity above the threshold\n",
    "    threshold = 0.779\n",
    "    similar_indices = (cosine_scores > threshold).nonzero().squeeze(dim=-1)\n",
    "\n",
    "    # Get the corresponding similar items\n",
    "    for idx in similar_indices:\n",
    "        similar_items.append(items[idx])  # Assuming 'items' is defined\n",
    "\n",
    "    return similar_items\n",
    "\n",
    "\n",
    "matrix = pd.read_csv('Final_co-occurence_polyvore_Adel.csv')\n",
    "\n",
    "\n",
    "def matrix_search_advice(attr, k):\n",
    "    match = []\n",
    "    i = 0\n",
    "    append = True\n",
    "\n",
    "    for a in matrix['bigram']:\n",
    "        if attr in a:\n",
    "            a = tuple(a[1:-1].replace('\\'', '').split(\", \"))\n",
    "\n",
    "            if attr in a[0]:\n",
    "                wrd = a[1]\n",
    "            else:\n",
    "                wrd = a[0]\n",
    "\n",
    "            remove = False\n",
    "            if (not wrd in match) and (not attr in wrd):\n",
    "                match.append(wrd)\n",
    "                i = i + 1\n",
    "            if i == k:\n",
    "                break\n",
    "\n",
    "    return match, i\n",
    "\n",
    "\n",
    "def matrix_search_match(attr, k):\n",
    "    match = []\n",
    "    i = 0\n",
    "\n",
    "    for a in matrix['bigram']:\n",
    "        if attr in a:\n",
    "            a = tuple(a[1:-1].replace('\\'', '').split(\", \"))\n",
    "\n",
    "            if attr in a[0]:\n",
    "                wrd = a[1]\n",
    "            else:\n",
    "                wrd = a[0]\n",
    "\n",
    "            remove = False\n",
    "            if (not wrd in match) and (not attr in wrd):\n",
    "                for el in match:\n",
    "                    # Calculate similarity using Sentence Transformers\n",
    "                    similarity_score = util.pytorch_cos_sim(\n",
    "                        model_sentence_transformer.encode(\n",
    "                            [el], convert_to_tensor=True),\n",
    "                        model_sentence_transformer.encode(\n",
    "                            [wrd], convert_to_tensor=True)\n",
    "                    )[0][0]\n",
    "\n",
    "                    if similarity_score > 0.7:\n",
    "                        remove = True\n",
    "\n",
    "                if not remove:\n",
    "                    match.append(wrd)\n",
    "                    i = i + 1\n",
    "            if i == k:\n",
    "                break\n",
    "\n",
    "    return match, i\n",
    "\n",
    "\n",
    "def garment_matching(attr, k):           # Returns k best matches to the given attribute\n",
    "\n",
    "    attr = \" \".join([lemmatizer.lemmatize(i) for i in attr.split()])\n",
    "    i = 0\n",
    "    match = []\n",
    "\n",
    "    if attr in items:\n",
    "        if k == 5:\n",
    "            match, i = matrix_search_match(attr, k)\n",
    "        if k == 10:\n",
    "            match, i = matrix_search_advice(attr, k)\n",
    "\n",
    "    else:\n",
    "        similar = similar_items(attr)\n",
    "        stop = False\n",
    "        ind = 0\n",
    "        while (not stop) and (ind < len(similar)):\n",
    "            print(len(similar))\n",
    "            if similar[ind] in items:\n",
    "                if k == 5:\n",
    "                    match, i = matrix_search_match(similar[ind], k)\n",
    "                if k == 10:\n",
    "                    match, i = matrix_search_advice(similar[ind], k)\n",
    "                if (i > 0):\n",
    "                    stop = True\n",
    "                    attr = similar[ind]\n",
    "            ind = ind + 1\n",
    "\n",
    "    if (i == 0):\n",
    "        message = 'This attribute was not found for the garment matching try another attribute!'\n",
    "        return message, []\n",
    "\n",
    "    return attr, match\n",
    "\n",
    "\n",
    "def garment_advice(attr1, attr2, k=10):\n",
    "    match, i = garment_matching(attr1, k)\n",
    "\n",
    "    if match is None:\n",
    "        return attr1, None, False\n",
    "\n",
    "    # Encode attr2 using Sentence Transformers\n",
    "    attr2_embedding = model_sentence_transformer.encode(\n",
    "        [attr2], convert_to_tensor=True)\n",
    "\n",
    "    for el in match:\n",
    "        # Calculate similarity using Sentence Transformers\n",
    "        similarity_score = util.pytorch_cos_sim(\n",
    "            model_sentence_transformer.encode([el], convert_to_tensor=True),\n",
    "            attr2_embedding\n",
    "        )[0][0]\n",
    "\n",
    "        if similarity_score > 0.9:\n",
    "            return attr1, el, True\n",
    "\n",
    "    return attr1, None, False\n",
    "\n",
    "    return None, None, False\n",
    "\n",
    "\n",
    "def check_image(num, attr1, attr2):\n",
    "\n",
    "    bound = eval(data['boudaries(X,y,Width,Height)'][num])\n",
    "    if (attr1 in bound.keys()) and (attr2 in bound.keys()):\n",
    "        x1, y1, x2, y2 = bound[attr1]\n",
    "        a1, b1, a2, b2 = bound[attr2]\n",
    "\n",
    "        percentage1 = (((x2-x1)/6) + ((y2-y1)/6)) / 2\n",
    "        percentage2 = (((a2-a1)/6) + ((b2-b1)/6)) / 2\n",
    "\n",
    "        center1 = np.array([x1 + (x2-x1)/2, y1 + (y2-y1)/2])\n",
    "        center2 = np.array([a1 + (a2-a1)/2, b1 + (b2-b1)/2])\n",
    "\n",
    "        dist = np.linalg.norm(center1 - center2)\n",
    "\n",
    "#         print(percentage1, percentage2, dist)\n",
    "#         print(center1, center2)\n",
    "\n",
    "        if percentage1 < 20 or percentage2 < 20:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        #print(\"One of the attributes is not found in the image\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def new_extract_image(attr1, attr2, k):\n",
    "    match = []\n",
    "    a = 0\n",
    "    for i, d in enumerate(data['Attributes']):\n",
    "        l = extract_from_sample(i)\n",
    "        if (attr1 in l) and (attr2 in l) and check_image(i, attr1, attr2):\n",
    "            matching_urls = list(frame[frame['id'] == i]['URL'])\n",
    "\n",
    "\n",
    "            if matching_urls:\n",
    "                match.append(matching_urls[0])\n",
    "            else:\n",
    "                # Handle the case when no URL is found for the current 'i'\n",
    "                match.append(\"No matching URL found for this ID\")\n",
    "\n",
    "            a = a + 1\n",
    "            if a == k:\n",
    "                break\n",
    "    # else :\n",
    "        #print(\"No image found\")\n",
    "\n",
    "    return match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10054] An existing connection was forcibly\n",
      "[nltk_data]     closed by the remote host>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def end_to_end(query):\n",
    "    responses = []  # Initialize an empty list to store responses\n",
    "\n",
    "    msg, attr = extraction_pipeline(query)\n",
    "\n",
    "    if attr is None:\n",
    "        responses.append(\n",
    "            \"An unknown problem occurred. Please contact support.\")\n",
    "    elif len(attr) == 1:  # Garment matching\n",
    "        attr0, match = garment_matching(attr[0], 5)\n",
    "        response = f\"{attr[0].capitalize()} will match with the following attributes:\"\n",
    "        responses.append(response)\n",
    "        for item in match:\n",
    "            responses.append(f\"- {item}\")\n",
    "\n",
    "        responses.append(\n",
    "            \"Here are some images of your item with some good matches:\")\n",
    "        for item in match:\n",
    "            URL = new_extract_image(attr0, item, 1)\n",
    "            URL = list(dict.fromkeys(URL))\n",
    "            responses.extend(URL)\n",
    "    elif len(attr) == 2:  # Garment advice\n",
    "        attr1, attr2, g = garment_advice(attr[0], attr[1], 10)\n",
    "        if g:\n",
    "            response = f\"{attr1.capitalize()} would be a good match with {attr2.capitalize()}.\"\n",
    "            responses.append(response)\n",
    "            responses.append(\"Here are some images of that combo:\")\n",
    "            URL = new_extract_image(attr1, attr2, 5)\n",
    "            URL = list(dict.fromkeys(URL))\n",
    "            responses.extend(URL)\n",
    "        elif attr1 is None:\n",
    "            responses.append(\"Those items are not commonly worn together!\")\n",
    "        else:\n",
    "            responses.append(attr1)\n",
    "    elif len(attr) == 0:\n",
    "        responses.append(msg)\n",
    "    else:\n",
    "        responses.append(\n",
    "            'More than 2 attributes were detected, this version only supports 1 attribute for garment matching and 2 for garment advice')\n",
    "\n",
    "    return responses  # Return a list of responses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyngrok in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyngrok) (6.0)\n",
      "Requirement already satisfied: streamlit in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.25.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (5.3.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (6.8.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (1.23.5)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (21.3)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (1.5.2)\n",
      "Requirement already satisfied: pillow<10,>=7.1.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (9.3.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (4.22.0)\n",
      "Requirement already satisfied: pyarrow>=6.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (12.0.1)\n",
      "Requirement already satisfied: pympler<2,>=0.9 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.18 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (2.28.1)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (13.3.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.1.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (4.7.1)\n",
      "Requirement already satisfied: tzlocal<5,>=1.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (4.2)\n",
      "Requirement already satisfied: validators<1,>=0.2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (0.21.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (3.1.31)\n",
      "Requirement already satisfied: pydeck<1,>=0.8 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (6.2)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (3.0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.19.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata<7,>=1.4->streamlit) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from packaging<24,>=16.8->streamlit) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.18->streamlit) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.18->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.18->streamlit) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.18->streamlit) (2022.9.24)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.14.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.14.0)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tzlocal<5,>=1.1->streamlit) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tzlocal<5,>=1.1->streamlit) (2022.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.9.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.1.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.66.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.9.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok\n",
    "!pip install streamlit\n",
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask) (2.2.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagar rahangdale\\appdata\\roaming\\python\\python311\\site-packages (from click>=8.0->Flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sagar rahangdale\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Jinja2>=3.0->Flask) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10054] An existing connection was forcibly\n",
      "[nltk_data]     closed by the remote host>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:3001\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Dec/2023 15:42:49] \"OPTIONS /prompt HTTP/1.1\" 200 -\n",
      "[2023-12-09 15:42:49,414] ERROR in app: Exception on /prompt [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py\", line 2077, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py\", line 1525, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask_cors\\extension.py\", line 165, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "                                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py\", line 1523, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py\", line 1509, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Temp\\ipykernel_8608\\1630515114.py\", line 101, in generate_prompt\n",
      "    bot_response = end_to_end(query)\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Temp\\ipykernel_8608\\3868243082.py\", line 4, in end_to_end\n",
      "    msg, attr = extraction_pipeline(query)\n",
      "                ^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'extraction_pipeline' is not defined\n",
      "127.0.0.1 - - [09/Dec/2023 15:42:49] \"POST /prompt HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What to wear with blue pant\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import util\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def recommend_product(sql, product_data):\n",
    "    # Convert LDJSON data to a DataFrame\n",
    "    productDf = pd.DataFrame(product_data)\n",
    "    \n",
    "    preprocessed_sql = sql.lower()\n",
    "    preprocessed_products = productDf.applymap(\n",
    "        lambda x: x.lower() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(preprocessed_sql)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Find adjectives and nouns and combine them\n",
    "    adjective_noun_pairs = []\n",
    "    i = 0\n",
    "    while i != len(pos_tags):\n",
    "        if pos_tags[i][1].startswith('JJ'):\n",
    "            j = i\n",
    "            while not pos_tags[j][1].startswith('NN'):\n",
    "                j += 1\n",
    "\n",
    "            r = []\n",
    "            for k in range(i, j + 1):\n",
    "                if pos_tags[k][1].startswith('JJ') or pos_tags[k][1].startswith('RB') or pos_tags[k][1].startswith('NN'):\n",
    "                    r.append(pos_tags[k][0])\n",
    "            adjective_noun_pairs.append(r)\n",
    "            i = j + 1\n",
    "\n",
    "        elif pos_tags[i][1].startswith('NN'):\n",
    "            adjective_noun_pairs.append([pos_tags[i][0]])\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    productDf['Combined'] = preprocessed_products['product_name'].str.cat(\n",
    "        preprocessed_products['product_name'], sep=' '\n",
    "    )\n",
    "\n",
    "    # Prepare the list of recommended products\n",
    "    recommended_products = []\n",
    "    for pair in adjective_noun_pairs:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        product_vectors = vectorizer.fit_transform(productDf['Combined'].fillna(''))\n",
    "        query_vector = vectorizer.transform([\" \".join(pair)])\n",
    "        similarity_scores = cosine_similarity(query_vector, product_vectors).flatten()\n",
    "\n",
    "        ranked_indices = similarity_scores.argsort()[::-1]\n",
    "        ranked_products = productDf.iloc[ranked_indices]\n",
    "\n",
    "        recommended_urls = ranked_products['product_url'].tolist()\n",
    "        recommended_prices = ranked_products['sales_price'].tolist()\n",
    "        recommended_names = ranked_products['product_name'].tolist()\n",
    "\n",
    "        recommendations = list(zip(recommended_urls[:10], recommended_prices[:10], recommended_names[:10]))\n",
    "        recommended_products.extend(recommendations)\n",
    "\n",
    "    recommended_products_list = []\n",
    "    for url, price, name in recommended_products:\n",
    "        recommended_product_dict = {\n",
    "            'name': name,\n",
    "            'url': url,\n",
    "            'price': price\n",
    "        }\n",
    "        \n",
    "        recommended_products_list.append(recommended_product_dict)\n",
    "\n",
    "    return recommended_products_list[:5]\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Load product data from LDJSON file with UTF-8 encoding\n",
    "with open('amazon.ldjson', 'r', encoding='utf-8') as ldjson_file:\n",
    "    product_data = [json.loads(line) for line in ldjson_file]\n",
    "\n",
    "@app.route('/prompt', methods=['POST'])\n",
    "def generate_prompt():\n",
    "    data = request.get_json()\n",
    "    query = data['query']\n",
    "    print(query)\n",
    "\n",
    "    \n",
    "\n",
    "    bot_response = end_to_end(query)\n",
    "    print(bot_response)\n",
    "    \n",
    "\n",
    "    response_to_send = recommend_product(query, product_data)\n",
    "    print(response_to_send)\n",
    "\n",
    "    return jsonify({\"answer\": response_to_send})\n",
    "    # return jsonify({\"answer\": bot_response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='127.0.0.1', port=3001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SAGAR\n",
      "[nltk_data]     RAHANGDALE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SAGAR RAHANGDALE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from flask import Flask, jsonify\n",
    "\n",
    "from flask_cors import CORS\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def recommendProduct(sql):\n",
    "    # Read the product dataset\n",
    "    productDf = pd.read_csv('articles.csv')\n",
    "    \n",
    "    preprocessedProducts = productDf.applymap(\n",
    "        lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    productDf['Combined'] = preprocessedProducts['product_type_name'].str.cat(\n",
    "        preprocessedProducts['graphical_appearance'], sep=' ').str.cat(\n",
    "        preprocessedProducts['colour_group_name'], sep=' ')\n",
    "\n",
    "    # Prepare the list of recommended products\n",
    "    recommended_products = []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    productVectors = vectorizer.fit_transform(productDf['Combined'].fillna(''))\n",
    "    query_vector = vectorizer.transform([\" \".join(sql)])\n",
    "    similarity_scores = cosine_similarity(query_vector, productVectors).flatten()\n",
    "\n",
    "    ranked_indices = similarity_scores.argsort()[::-1]\n",
    "    ranked_products = productDf.iloc[ranked_indices]\n",
    "\n",
    "    recommended_urls = ranked_products['product_url'].tolist()\n",
    "    recommended_prices = ranked_products['retail_price'].tolist()\n",
    "    recommended_names = ranked_products['product_name'].tolist()\n",
    "    recommended_img_url = ranked_products['image'].tolist()\n",
    "    #print(recommended_img_url)\n",
    "    recommendations = []\n",
    "    recommendations.extend(list(zip(recommended_urls[:10], recommended_prices[:10], recommended_names[:10])))\n",
    "\n",
    "    recommended_products.extend(recommendations)\n",
    "\n",
    "    recommended_products_list = []\n",
    "    for url, price, name in recommended_products:\n",
    "        recommended_product_dict = {\n",
    "            'name': name,\n",
    "            'url': url,\n",
    "            'price': price\n",
    "        }\n",
    "        \n",
    "        recommended_products_list.append(recommended_product_dict)\n",
    "\n",
    "    return recommended_products_list[:5]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:6033\n",
      " * Running on http://10.40.5.226:6033\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Dec/2023 14:19:11] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Dec/2023 14:19:11] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "c:\\Users\\SAGAR RAHANGDALE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [08/Dec/2023 14:19:17] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Black shirt will match with the following attributes:', '- black heel', '- black jacket', '- black boot', '- black pant', '- black hat', 'Here are some images of your item with some good matches:', 'No matching URL found for this ID', 'No matching URL found for this ID', 'No matching URL found for this ID', 'No matching URL found for this ID', 'No matching URL found for this ID']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Dec/2023 14:19:46] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Working ...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Dec/2023 14:20:03] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Working ...']\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "# Import your functions and necessary libraries here...\n",
    "from sentence_transformers import util  # Import the necessary libraries\n",
    "# ...\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def chatbot():\n",
    "    if request.method == 'POST':\n",
    "        user_message = request.form.get('user_input')\n",
    "        if user_message:\n",
    "            bot_response = end_to_end(\n",
    "                user_message)  # Process user's message\n",
    "            \n",
    "            # ans_response= recommendProduct(bot_response[2])\n",
    "\n",
    "            \n",
    "            print(bot_response)\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(ans_response)    \n",
    "                \n",
    "            return render_template('index.html', user_message=user_message, bot_response=bot_response)\n",
    "\n",
    "    return render_template('index.html', user_message='', bot_response='')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=6033, debug=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
